# [论文阅读] 神经网络统计语言模型

[论文原文](http://www.jmlr.org/papers/v3/bengio03a.html)

## 0. 摘要
统计语言模型的一个目标是为了学习单词组成句子的联合概率函数。由于“维数灾难”，这一目标难以完成：测试集中可能出现训练集中不存在的词序列。传统的方法是n元语言模型：基于较短的、重叠的短序列建立联合概率模型。我们的目标是通过学习单词的分布式表达来对抗维数灾难。这一方法允许每个训练句告诉模型大量的语意邻句。这一模型同步地学习：
	（1）每个词的分布式表达
	（2）词序列基于这些词表达的概率函数
该模型会根据生成与现有句子在语意空间上相近的句子。训练这样一个大模型（百万量级的参数）是一个巨大的挑战。本文报告了使用神经网络训练概率函数的实验结果，结果表明在两个文本语料库中，该方法显著的提高了性能，该方法能够利用长文本信息。
## 1. 引言
维数灾难是使得语言建模以及其他学习问题变得困难的一个根本问题。当我们想要对一堆离散随机变量的联合分布进行建模时，该问题变得尤为明显。例如，如果一个人想要对大小为100000的语料库中长度为10的词序列的联合概率分布进行建模，大概存在$100000^{10}−1=10^{50}−1$个自由参数。而对连续变量建模时，我们可以更容易的获取泛化模型（例如使用类似平滑类别的多层神经网络或高斯混合模型），因为被学习的函数可以被认为具有一些局部的平滑属性。对于离散空间，这种泛化结构并不明显：这些离散变量的任何改变都会对被评估的函数造成巨大影响，而每个离散变量的取值数量都可能非常巨大。

对不同学习算法是如何泛化进行可视化的一个有效方法（来自非参数密度估计）是观察概率质量（初始时聚焦于训练样本上）是如何在大体量中分布的，通常聚焦于训练点的邻域。在高维度中，直观重要的是将概率质量分配到重要的方向，而不是在每个样本点周围均匀分布。我们在这片论文中展示了，所述方法的泛化过程与其他统计语言模型的泛化过程有根本上的不同。

一个统计语言模型可以用下一个词的条件概率表达：
$$
P ̂(w_1^T)= \prod_{t=1}^T\hat{P}{(w_t│w_1^{t−1})}
$$

其中$w_t$是第t个单词，而子序列$w^j_i=(w_i,w_{i+1},…,w_{j−1},w_j)$。这种统计语言模型已经被证明在很多自然语言相关技术应用中非常有用，例如语音识别、语言翻译、信息检索。对统计语言模型的提高可以对这种应用有显著的提高。

当构建自然语言统计模型时，一个可以减少建模困难的思考方向是利用词序，在词序列中越接近的词在统计上越相关。基于这一事实，n元语言模型基于前n-1个词建立下一个词的条件概率。
$$
\hat{P}(w_t│w_1^{t−1})≈\hat{P}(w_t│w_{t−n+1}^{t−1})
$$
我们只考虑了出现在训练语料库中的连续词组合。当一个新的n词组合（训练语料库中不存在）出现时，会发生什么？对于这种情况，我们并不想分配一个零概率，因为这种新组合很可能出现，并且在更大的语料库中更频繁的出现。一个简单的方法是使用一个更小的context长度，就像back-off trigram models(Katz, 1987)和in smoothed trigram models（Jelinek and Mercer, 1980）。所以，这些模型是如何从训练集的词序列中获得新词序列的基础泛化？一个理解这一点的方法是思考一个与这些interpolated或backoff n-gram模型对应的生成模型。从本质上来说，一个新的词序列是通过“gluing”非常短的、重叠的、经常出现在训练集中的词片段来实现的。获取下一词片段的概率的规则是通过back-off或interpolated n-gram算法推测的。传统的研究者使用n=3的trigrams算法获得了当前的state-of-the-art的结果，但是看看Goodman(2001)，要包含多少tricks，才能够产生至关重要的性能提升。很明显，序列中还有很多重要信息可以用来预测单词，而不是仅仅使用前几个词。在这种方法中至少存在两个方面需要提高：
    
    1. 该方法没有考虑更远的单词。
    2. 该方法没有考虑词之间的相似性。

例如，看到"The cat is walking in the bedroom" 应该能帮助产生"A dog was running in a room"，因为cat和dog相似，而bedroom和room相似，因为这些词具有语意和语法角色上的相似性。

现有研究已经提出了很多方法来解决这两个问题，我们将在1.2节对这些方法进行简单的解释和说明。我们将首先讨论所提出方法的基本思想。在第2节中会展示一个更正式的表达，基于参数共享的多层神经网络完成一个实现。这片文章的另一个贡献是关注了如何基于一个大数据集（数百万或数千万样本）训练一个大神经网络（数百万参数）。最后，本文的一个重要贡献是展示了训练这样一个大规模模型的代价是昂贵的但是是可行的，能够产生相当好的结果（第4节）。

本文的很多操作是矩阵操作，小写字母v表示一个列向量，$v'$表示其转置，$A_j$ 表示矩阵A的第j行，$x⋅y=x^′y$

### 1.1 用分布式表达对抗维数灾难
简单的说，我们的方法可以如下总结：

    1. 将词库中的每个词关联一个分布式的词特征向量，实值向量∈R^m
    2. 用这些词的特征向量来表达词序列的邻接概率函数
    3. 同步学习词特征向量和概率函数的参数

特征向量表达了词的不同方面：每个词都对应了一个向量空间中的一个点。特征的数量（在实验中设置为30、60、100）通常比词库的大小（例如17000）小很多。该概率函数表达为前面的词预测后面的词的条件概率的乘积。这一函数的参数可以通过最大化对数似然来迭代调整。每个词对应的特征向量通过学习得到，但是其初值可以通过先验的语意特征进行初始化。

为什么这一方法能够可行？在前面的例子中，如果我们知道了dog和cat扮演了相似的角色（语意、语法），类似得还有（the，a），（bedroom,room),(is,was),(running,walking)，我们能够自然的进行推广，依据
	
	The cat is walking in the bedroom

推广到：
	
	A dog was running in a room
	The cat is running in a room

等等。

在提出的模型中，它会这样进行推广是因为相似的词会有相近的特征向量，而且因为概率函数是这些特征值的光滑函数，在特征值上的微小改变会导致在概率上的微小改变。因此，上面提到的句子只要有一个存在，不仅会增加该句子本身出现的概率，同时也会增加其在句子空间中的邻居的概率。

### 1.2 先前研究
使用神经网络对高维度离散分布进行建模的思想已经被证明在学习一系列具有不同本质的随机变量的联合概率时非常有效（Bengio and Bengio,2000a,b）。在那个模型中，联合概率分布被分解为一系列条件概率的乘积
$$
P ̂(Z_1=z_1,…,Z_n=z_n )=\prod_i{\hat{P}(Z_i=z_i|g_(i ) (Z_{i−1}=z_{i−1},Z_{i−2}=z_{i−2},…,Z_1=z_1 ))}
$$

其中，$g(.)$是一个函数，由一个特殊的left-to-right结构的神经网络表示，其第i个输出表示为$g_i()$表达了由$Z_i$之前的随机变量Z到$Z_i$的条件概率。该方法在四个UCI数据集上测试结果很好。但在本文中，我们必须考虑随机变量的数据长度，因此上述方法需要进行更改。另一个重要的不同是，所有的$Z_i$指的是具有相同类型的对象。这里提出的模型因此引入了一种基于时间的参数共享——即随着时间变化，使用相同的$g_i$，这里的时间是指输入的词序列中的不同位置。这和连结主义学习符号数据的分布式表达的思想相同（Hinton, 1986, Elman 1990）。更近一些，Hinton的方法被进一步提高，并在几个符号关系中进行了成功演示（Paccanaro and Hinton, 2000）。使用神经网络进行语言建模的思想也不是新的（Miikkulainen and Dyer, 1991）。但是，我们将这种思想推向了大规模，聚焦于学习词序列分布的统计模型，而不是学习词在句中的角色。本文提出的方法也来自于先前提出的基于字符的文本压缩——使用神经网络来预测下一个字符出现的概率（Schmidhuber, 1996）。使用神经网络进行语言建模的思想同样也被（Xu and Rudnicky, 2009）独立地提出，尽管其实验的网络并不包含隐层，输入也只是一个单词，这限制了其模型学习更多的统计特性。

通过发现词的相似性来获取泛化能力，生成新句的思想不是新的。例如，基于学习的词聚类方法（Brown et al., 1992, Pereira et al., 1993, Niesler et al., 1998, Baker and McCallum, 1998）:。。。。

使用向量空间表达单词的细想已经在信息检索中被很好的使用（Schutze，1993），其中。。。

## 2. 一个神经模型
训练集是由词库中的词$w_t∈V$组成的序列$w_1…W_T$,其中语料库V是一个庞大但有限的集合。目标是为了学习一个优秀的模型$f(w_t,...,w_{t−n+1})=P ̂(w_t│w_1^{t−1})$,即有优秀的泛化能力。下面，我们报道了$1/(\hat{P}(w_t│w_1^{t−1}))$ 的几何平均，又叫做perplexity，也是平均对数似然的指数。模型唯一的约束是：
	
对任意的$w_1^{t−1}$,$\sum_{i=1}^{|V|}f(i,w_{t−1},...,w_{t−n+1})=1, f>0.$

通过这些条件概率的乘积，我们可以获得一个关于词序列的联合概率的模型。

我们将函数$f(w_t,…w_(t−n+1) )=\hat{P}(w_t│w_1^{t−1})$分解为两个部分：

1. 一个映射关系C将每个单词i∈V映射到一个实值向量$C(i)∈R^m$。它代表了每个词在词库中的分布式特征向量。在实际中，C代表一个$|V|∗m$的参数矩阵
2. 一个关于每个单词的概率函数，由C表达：一个函数g将一个输入的特征向量序列$（C(w_{t−n+1}),…,C(w_{t−1})$映射到一个条件概率分布，该条件分布指示出在V中的每个词成为下个词的概率。g的输出是一个向量，向量的第i个元素评估了$\hat{P}(w_t=i|w_1^{t−1})$，如图1所示。

![](https://i.imgur.com/yLpnDUY.jpg)

$$
f(w_t,…w_{t−n+1})=g(i, C(w_{t−n+1}),…,C(w_{t−1}))
$$

函数f是上述两个映射（C和g）的组合，其中C在所有单词中共享参数。这两个部分都关联了一些参数。映射C的参数就是单词的特征向量自身，由一个$|V|∗m$的矩阵C表示，C的第i行表示单词i的特征向量$C(i)$。函数g可以被实现为一个前馈神经网络或循环神经网络或者其他的带参函数，设参数为$ω$。因此，全部的参数集为$θ=(C, ω)$。

训练目标是找到θ来最大化训练词库中的带罚项的对数似然：
$$
L=\frac{1}{T}\sum_{t}log⁡(f(w_t, w_{t−1},…,w_{t−n+1};θ)) +R(θ)
$$
其中$R(θ)$表示正则项。例如，在我们的实现中，R是一个weight decay penalty， 只应用于神经网络的权重和C矩阵，而不应用于偏置。

在上述模型中，自由参数的数量是|V|的线性规模，同时也是句长n的线性规模：如果引入更多的共享结构，规模因子可以进一步减小。

在下面的大多数实验中，神经网络在词向量后只有一个隐层，或者直接将词向量连结到输出层。因此，实际上，这里包含了两个隐层，一个共享参数的词向量层C，该层不包含非线性函数，以及一个普通双曲线正切函数的隐层。更精确地说，神经网络计算了如下函数，并使用了一个softmax输出层来保证输出概率为正，且概率和为1：
$$
\hat{P}(w_t│w_{t−1},...,w_{t−n+1})=\frac{e^{y_{w_t}}}{\sum_i{e^{y_i}}}
$$
其中y_i 是每个输出单词i的无正则项的log-probabilities，计算如下：
$$
y=b+Wx+Utanh(d+Hx)
$$
其中双曲正切函数被应用到每个元素，W可以为0，x是词向量层的激活向量，是矩阵C中所有输入词向量的串联。
$$
x=(C(w_(t−n+1) ),…,C(w_(t−1) ))
$$
设h是隐单元的数量，m是每个词向量的特征数量。当不希望词向量直接连结到输出时，W为0。模型的自由参数为输出偏置b（|V|个元素），隐层偏置d（h个元素），隐层到输出的权重U（一个$|V|*h$的矩阵），词向量到输出的权重W（一个$|V|*(n-1)m$的矩阵），隐层权重H（一个$h*(n-1)m$的矩阵），以及词特征矩阵C（一个$|V|*m$的矩阵）：
$$
			θ=(b,d,W,U,H,C)
$$

自由参数的数量为$|V|(1+nm+h+h(1+(n−1)m)$，其中主要因素为$|V|(nm+h)$。在理论上，如果H和W上有衰减，而C无衰减，则H和W可能衰减到0，此时C会被放大。但在实际中，我们使用随机梯度上升进行训练并没有观察到这种现象。

在神经网络上进行随机梯度上升的迭代过程如下：
$$
\theta \leftarrow \theta+\varepsilon\frac{\partial log{\hat{P}(w_t|w_{t-1},...,w_{t-n+1})}}{\partial{\theta}}
$$
其中$ε$为学习率。

###混合模型
在实验中，我们发现通过将神经网络概率预测函数与差值trigram模型组合，两个模型分别给予0.5的权重，可以提升性能。

下面是并行计算方法和实验以及结果，不翻译了。






